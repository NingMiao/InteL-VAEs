{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample):\n",
    "    #Cluster number: 2\n",
    "    return tf.stack([tf.cast(tf.math.sign(sample[:,0]), tf.float32)*tf.abs(sample[:,0])**0.2*5+sample[:,0], sample[:,1]], axis=1)\n",
    "    #return tf.stack([sample[:,1], tf.cast(tf.math.sign(sample[:,0]), tf.float32)+sample[:,0]], axis=1)\n",
    "    #return tf.stack([sample[:,0], tf.cast(tf.math.sign(sample[:,1]), tf.float32)+sample[:,1]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size=2\n",
    "z_size=2\n",
    "mapping=False\n",
    "\n",
    "Vamp=False\n",
    "Vamp_input_num=10 #!\n",
    "\n",
    "class coder(Model):\n",
    "    def __init__(self, output_size, transform=None):\n",
    "        super(coder, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.d3 = tf.keras.layers.Dense(output_size*2, activation=None)\n",
    "        self.transform=transform\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        return x\n",
    "\n",
    "encoder = coder(z_size)\n",
    "if mapping:\n",
    "    decoder = coder(x_size, transform=transform)\n",
    "else:\n",
    "    decoder = coder(x_size, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, axis=-1):\n",
    "    x_max=tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    x-=x_max\n",
    "    x_exp=tf.exp(x)\n",
    "    x_exp_mean=tf.reduce_sum(x_exp, axis=axis)\n",
    "    x_exp_mean_log=tf.math.log(x_exp_mean)+tf.squeeze(x_max, axis=-1)\n",
    "    return x_exp_mean_log\n",
    "def logmeanexp(x, axis=-1):\n",
    "    return logsumexp(x, axis=axis)-tf.math.log(float(x.shape[axis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logP_MoG(x, mean, log_var):\n",
    "    x=tf.expand_dims(x, 1)\n",
    "    mean=tf.expand_dims(mean, 0)\n",
    "    log_var=tf.expand_dims(log_var, 0)\n",
    "    logP_component_dim=-np.log(2*np.pi)/2-log_var/2-(x-mean)**2/(2*tf.exp(log_var))\n",
    "    logP_component=tf.reduce_sum(logP_component_dim, axis=-1)\n",
    "    #P_component=tf.exp(logP_component)\n",
    "    #P=tf.reduce_mean(P_component, axis=-1)\n",
    "    #logP=tf.math.log(P)\n",
    "    logP=logmeanexp(logP_component, axis=-1)\n",
    "    return logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Vamp:\n",
    "    initial_value=np.random.normal(0,1,[Vamp_input_num, x_size]).astype(np.float32)\n",
    "    pseduo_input=tf.Variable(initial_value=initial_value, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "reconstruction_logprob_record = tf.keras.metrics.Mean(name='reconstruction')\n",
    "KL_term_record = tf.keras.metrics.Mean(name='KL')\n",
    "ELBO_record = tf.keras.metrics.Mean(name='ELBO')\n",
    "\n",
    "def sample(mean, log_var):\n",
    "    batch = tf.shape(mean)[0]\n",
    "    dim = tf.shape(mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "def reconstruction_logprob(x, x_mean, x_log_var):\n",
    "    dim = tf.shape(x)[1]\n",
    "    return -tf.cast(dim, tf.float32)*0.5*(tf.math.log(2.0*np.pi)+tf.reduce_sum(x_log_var, axis=1))-0.5*tf.reduce_sum((x-x_mean)**2*tf.exp(-x_log_var), axis=1)\n",
    "\n",
    "def KL_term(mean_1, log_var_1, mean_2, log_var_2):\n",
    "    dim_KL = 0.5*(log_var_2-log_var_1)+(tf.exp(log_var_1)+(mean_1-mean_2)**2)/(2*tf.exp(log_var_2))-0.5\n",
    "    return tf.reduce_sum(dim_KL, axis=1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, beta=1e-7, test=False): #!\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_x_info = encoder(x)\n",
    "        z_x_mean=z_x_info[:,:z_size]\n",
    "        z_x_log_var=z_x_info[:,z_size:]\n",
    "        z_sample=sample(z_x_mean, z_x_log_var)\n",
    "        x_z_info=decoder(z_sample)\n",
    "        x_z_mean=x_z_info[:,:x_size]\n",
    "        x_z_log_var=x_z_info[:,x_size:]\n",
    "        reconstruction_logprob_0=tf.reduce_mean(reconstruction_logprob(x, x_z_mean, x_z_log_var))\n",
    "        \n",
    "        if not Vamp:\n",
    "            KL_term_0=tf.reduce_mean(KL_term(z_x_mean, z_x_log_var, z_x_mean*0, z_x_log_var*0+1))\n",
    "            \n",
    "            #logQ = reconstruction_logprob(z_sample, z_x_mean, z_x_log_var)\n",
    "            #logP = logP_MoG(z_sample, z_sample[0:1]*0, z_sample[0:1]*0)\n",
    "            #logP = log_normal_pdf(z_sample, z_sample*0, z_sample*0)\n",
    "            \n",
    "            #KL_term_0 = tf.reduce_mean(logQ-logP)\n",
    "        \n",
    "        else:\n",
    "            pseduo_z_x_info = encoder(pseduo_input)\n",
    "            pseduo_z_x_mean = pseduo_z_x_info[:,:z_size]\n",
    "            pseduo_z_x_log_var = pseduo_z_x_info[:,z_size:]\n",
    "            #pseduo_z_x_mean = pseduo_z_x_info[0:1,:z_size] #!\n",
    "            #pseduo_z_x_log_var =pseduo_z_x_info[0:1,:z_size] #!\n",
    "            logQ = reconstruction_logprob(z_sample, z_x_mean, z_x_log_var)\n",
    "            logP = logP_MoG(z_sample, pseduo_z_x_mean, pseduo_z_x_log_var)\n",
    "            KL_term_0 = tf.reduce_mean(logQ-logP) ##Problem here\n",
    "        \n",
    "        gain = reconstruction_logprob_0 - KL_term_0*beta\n",
    "        loss = -gain\n",
    "    if not test:\n",
    "        trainable_variables=encoder.trainable_variables+decoder.trainable_variables\n",
    "        if Vamp:\n",
    "            trainable_variables.append(pseduo_input)\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    reconstruction_logprob_record(reconstruction_logprob_0)\n",
    "    KL_term_record(KL_term_0)\n",
    "    ELBO_record(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_mat_45=np.array([[2**0.5, 2**0.5],[2**-0.5, -2**-0.5]]).astype(np.float32)\n",
    "transform_mat_0=np.array([[1.0, 0.0],[0.0, 1.0]]).astype(np.float32)\n",
    "transform_mat_1_dim=np.array([[1, 0],[0, 0.01]]).astype(np.float32)\n",
    "\n",
    "\n",
    "def data_generation_Gaussian(batch_size=10, transform_mat=transform_mat_0):\n",
    "    Gaussian=np.random.normal(0, 1, [batch_size, 2]).astype(np.float32)\n",
    "    return np.dot(Gaussian, transform_mat)\n",
    "\n",
    "def data_generation_uniform(batch_size=10, dim=2):\n",
    "    Uniform=np.random.uniform(-1,1,[batch_size, dim]).astype(np.float32)*2**0.5\n",
    "    return Uniform\n",
    "\n",
    "def data_generation_circle(batch_size=1, portion=1):\n",
    "    r=np.random.uniform(0, 2*np.pi*portion, [batch_size])\n",
    "    \n",
    "    Circle_cos=np.cos(r)\n",
    "    Circle_sin=np.sin(r)\n",
    "    Circle=np.stack([Circle_cos,Circle_sin], axis=1).astype(np.float32)*2\n",
    "    return Circle\n",
    "\n",
    "def data_generation_MoG(batch_size=10):\n",
    "    Gaussian_1=np.random.normal(0, 0.3, [int(batch_size/2), 2]).astype(np.float32)\n",
    "    Gaussian_1[:,0]-=2\n",
    "    Gaussian_2=np.random.normal(0, 0.3, [int(batch_size/2), 2]).astype(np.float32)\n",
    "    Gaussian_2[:,0]+=2\n",
    "    return np.concatenate([Gaussian_1, Gaussian_2])\n",
    "\n",
    "def data_generation(batch_size=10, seed=0):\n",
    "    if seed!=0:\n",
    "        np.random.seed(seed)\n",
    "    return data_generation_MoG(batch_size=batch_size)\n",
    "    #return data_generation_Gaussian(batch_size=batch_size)\n",
    "\n",
    "X_data=data_generation(1000)\n",
    "fig = plt.figure(frameon=False)\n",
    "plt.plot(X_data[:,0], X_data[:,1], '.')\n",
    "plt.axis([-4.5,4.5, -3, 3])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "fig.savefig('figure/MoG_real.png', bbox_inches = 'tight',\n",
    "    pad_inches = 0) #!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_record = tf.keras.metrics.Mean(name='logP')\n",
    "\n",
    "def reparameterize_multi(mean, logvar, num=10):\n",
    "    shape=mean.shape\n",
    "    shape_new=[mean.shape[0], num]\n",
    "    for i in range(1, len(shape)):\n",
    "        shape_new.append(shape[i])\n",
    "    eps = tf.random.normal(shape=shape_new)   \n",
    "    return eps * tf.exp(tf.expand_dims(logvar, axis=1) * .5) + tf.expand_dims(mean, axis=1)\n",
    "    \n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "def compute_log_P(x, num=10):\n",
    "    x_shape = x.shape\n",
    "    z_x_info = encoder(x)\n",
    "    mean=z_x_info[:,:z_size]\n",
    "    logvar=z_x_info[:,z_size:]\n",
    "\n",
    "    z = reparameterize_multi(mean, logvar, num)\n",
    "    mean_multi=tf.tile(tf.expand_dims(mean, axis=1), multiples=[1,num,1])\n",
    "    log_var_multi=tf.tile(tf.expand_dims(logvar, axis=1), multiples=[1,num,1])\n",
    "    z_logprob_in_log_z_x=log_normal_pdf(z, mean_multi, log_var_multi, raxis=-1)\n",
    "    \n",
    "    if not Vamp:\n",
    "        z_logprob_in_log_z=log_normal_pdf(z, mean_multi*0, log_var_multi*0, raxis=-1)\n",
    "    else:\n",
    "        pseduo_z_x_info = encoder(pseduo_input)\n",
    "        pseduo_z_x_mean = pseduo_z_x_info[:,:z_size]\n",
    "        pseduo_z_x_log_var = pseduo_z_x_info[:,z_size:]\n",
    "        z_reshape=tf.reshape(z, [-1,z_size])\n",
    "        \n",
    "        z_logprob_in_log_z=logP_MoG(z_reshape, pseduo_z_x_mean, pseduo_z_x_log_var)\n",
    "        z_logprob_in_log_z=tf.reshape(z_logprob_in_log_z, [-1, num])\n",
    "    \n",
    "    logratio=z_logprob_in_log_z-z_logprob_in_log_z_x\n",
    "\n",
    "    z_reshape = tf.reshape(z, [-1, z.shape[-1]])\n",
    "                    \n",
    "    x_z_info=decoder(z_reshape)\n",
    "    x_z_mean=x_z_info[:,:x_size]\n",
    "    x_z_logvar=x_z_info[:,x_size:]\n",
    "    x_z_mean=tf.reshape(x_z_mean, [-1, num, x_shape[1]])\n",
    "    x_z_logvar=tf.reshape(x_z_logvar, [-1, num, x_shape[1]])\n",
    "    \n",
    "    x_multi=tf.tile(tf.expand_dims(x,1),multiples=[1,num,1])\n",
    "    logP_x_z=log_normal_pdf(x_multi, x_z_mean, x_z_logvar, raxis=-1)\n",
    "    logP_pre=logP_x_z+logratio\n",
    "    logP_pre_max=tf.reduce_max(logP_pre, axis=1)\n",
    "    logP_pre_sub=logP_pre-tf.expand_dims(logP_pre_max, axis=1)\n",
    "    logP_x=tf.math.log(tf.reduce_mean(tf.exp(logP_pre_sub), axis=1))+logP_pre_max\n",
    "    logP_record(logP_x)\n",
    "\n",
    "def logP_epoch(data_generation_function, num, sample_size):\n",
    "    logP_record.reset_states()\n",
    "    i=0\n",
    "    while True:\n",
    "        test_x=data_generation_function(batch_size=10, seed=101)\n",
    "        compute_log_P(test_x, num)\n",
    "        i+=test_x.shape[0]\n",
    "        if i>=sample_size:\n",
    "            break\n",
    "    return logP_record.result().numpy()\n",
    "logP_epoch(data_generation, 500, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "beta=1\n",
    "for epoch in range(EPOCHS):\n",
    "    reconstruction_logprob_record.reset_states()\n",
    "    KL_term_record.reset_states()\n",
    "    ELBO_record.reset_states()\n",
    "\n",
    "    for i in range(10):    \n",
    "        X_data=data_generation()\n",
    "        train_step(X_data, beta=beta)\n",
    "\n",
    "    template = 'Epoch {}, ELBO:{}, reconstruct: {}, KL: {}'\n",
    "    if epoch%100==0:\n",
    "        print (template.format(epoch+1,\n",
    "                          ELBO_record.result(), \n",
    "                          reconstruction_logprob_record.result(),\n",
    "                          KL_term_record.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_epoch(data_generation, 500, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_MoG(batch_size, mean, log_var):\n",
    "    samples=[]\n",
    "    batch_size_each_component=batch_size//mean.shape[0]\n",
    "    for i in range(mean.shape[0]):\n",
    "        sample=np.random.normal(0, 1, [batch_size_each_component, mean.shape[1]]).astype(np.float32)\n",
    "        sample=sample*np.exp(log_var[i]*0.5)+mean[i]\n",
    "        samples.append(sample)\n",
    "    samples=np.concatenate(samples, axis=0)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Vamp:\n",
    "    Z_data=data_generation_Gaussian(200, transform_mat=transform_mat_0)\n",
    "else:\n",
    "    pseduo_z_x_info = encoder(pseduo_input)\n",
    "    pseduo_z_x_mean = pseduo_z_x_info[:,:z_size]\n",
    "    pseduo_z_x_log_var = pseduo_z_x_info[:,z_size:]\n",
    "    Z_data=generate_from_MoG(200, pseduo_z_x_mean, pseduo_z_x_log_var)\n",
    "x_z_info = decoder(Z_data)\n",
    "x_z_mean=x_z_info[:,:x_size]\n",
    "x_z_log_var=x_z_info[:,x_size:]\n",
    "x_z_std=np.exp(x_z_log_var/2)\n",
    "samples=sample(x_z_mean, x_z_log_var)\n",
    "\n",
    "fig = plt.figure(frameon=False)\n",
    "plt.plot(samples[:,0], samples[:,1], '.')\n",
    "plt.axis([-4.5,4.5, -3, 3])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "#fig.savefig('figure/cluster_vamp.pdf', bbox_inches = 'tight',\n",
    "#    pad_inches = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
